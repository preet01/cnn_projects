{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.read_csv('train.csv')\n",
    "# test=pd.read_csv('test.csv')\n",
    "\n",
    "# ytrain=train['label']\n",
    "# xtrain=train.drop(['label'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape((-1,28,28,1))\n",
    "x_test=x_test.reshape((-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaling\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train/=255.0\n",
    "x_test/=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr,xval,ytr,yval=train_test_split(x_train,y_train,test_size=0.1,random_state=2)#0.3 test size try that too\n",
    "#but this can also cause errors by causing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4f3560e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xtr[99][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def mnist_model(X,y,is_training):\n",
    "    \n",
    "    conv1=tf.layers.conv2d(inputs=X,filters=32,kernel_size=[5,5],padding='same',activation=tf.nn.relu)\n",
    "    conv2=tf.layers.conv2d(inputs=conv1,filters=32,kernel_size=[5,5],padding='same',activation=tf.nn.relu)\n",
    "    pool1=tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2)\n",
    "    \n",
    "    layer_flat=tf.contrib.layers.flatten(pool1)\n",
    "    \n",
    "    dense1=tf.contrib.layers.fully_connected(layer_flat,num_outputs=64,activation_fn=tf.nn.relu)#class scores which\n",
    "    \n",
    "    y_out=tf.contrib.layers.fully_connected(dense1,num_outputs=10,activation_fn=None)\n",
    "    \n",
    "    return y_out       \n",
    "    \n",
    "y_out = mnist_model(X,y,is_training)\n",
    "\n",
    "mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=(tf.one_hot(y,10)),logits=y_out))\n",
    "#y is the real value and y_out is the predicted\n",
    "\n",
    "#the y_out is a class score converted to 10 outputs which can be processed in to calcualate the softmax loss.\n",
    "#Now the softmax loss and softmax classifier are two really separate things.\n",
    "#this is loss calcualating using correct labels and predictions\n",
    "\n",
    "optimizer=tf.train.RMSPropOptimizer(1e-3)\n",
    "\n",
    "train_step=optimizer.minimize(mean_loss)#gradient changes are performed with this\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session,predict,loss_val,Xd,yd,epochs=1,batch_size=64,print_every=100,training=None,plot_losses=False):\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    prediction=tf.argmax(predict,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    #variables = [mean_loss,correct_prediction,accuracy]\n",
    "    variables = [mean_loss,correct_prediction,prediction]\n",
    "    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        y_pred= []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, prediction = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            y_pred.append(prediction)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct,y_pred\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "Iteration 0: with minibatch training loss = 2.31 and accuracy of 0.062\n",
      "Iteration 200: with minibatch training loss = 0.181 and accuracy of 0.94\n",
      "Iteration 400: with minibatch training loss = 0.171 and accuracy of 0.97\n",
      "Iteration 600: with minibatch training loss = 0.0125 and accuracy of 1\n",
      "Iteration 800: with minibatch training loss = 0.0543 and accuracy of 0.98\n",
      "Epoch 1, Overall loss = 0.349 and accuracy of 0.884\n",
      "Iteration 1000: with minibatch training loss = 0.227 and accuracy of 0.98\n",
      "Iteration 1200: with minibatch training loss = 0.0813 and accuracy of 0.98\n",
      "Iteration 1400: with minibatch training loss = 0.00573 and accuracy of 1\n",
      "Iteration 1600: with minibatch training loss = 0.00103 and accuracy of 1\n",
      "Epoch 2, Overall loss = 0.0437 and accuracy of 0.987\n",
      "Iteration 1800: with minibatch training loss = 0.0378 and accuracy of 0.98\n",
      "Iteration 2000: with minibatch training loss = 0.00953 and accuracy of 1\n",
      "Iteration 2200: with minibatch training loss = 0.0327 and accuracy of 0.98\n",
      "Iteration 2400: with minibatch training loss = 0.0217 and accuracy of 0.98\n",
      "Epoch 3, Overall loss = 0.0279 and accuracy of 0.992\n",
      "Iteration 2600: with minibatch training loss = 0.00737 and accuracy of 1\n",
      "Iteration 2800: with minibatch training loss = 0.00144 and accuracy of 1\n",
      "Iteration 3000: with minibatch training loss = 0.135 and accuracy of 0.98\n",
      "Iteration 3200: with minibatch training loss = 0.0128 and accuracy of 1\n",
      "Epoch 4, Overall loss = 0.0196 and accuracy of 0.995\n",
      "Iteration 3400: with minibatch training loss = 8.66e-05 and accuracy of 1\n",
      "Iteration 3600: with minibatch training loss = 0.00316 and accuracy of 1\n",
      "Iteration 3800: with minibatch training loss = 0.00219 and accuracy of 1\n",
      "Iteration 4000: with minibatch training loss = 6.48e-05 and accuracy of 1\n",
      "Iteration 4200: with minibatch training loss = 0.000286 and accuracy of 1\n",
      "Epoch 5, Overall loss = 0.0141 and accuracy of 0.997\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"training\")\n",
    "_,_,_=run_model(sess,y_out,mean_loss,xtr,ytr,5,64,200,train_step)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "Epoch 1, Overall loss = 0.0495 and accuracy of 0.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"validation\")\n",
    "_,_,_=run_model(sess,y_out,mean_loss,xval,yval,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "Epoch 1, Overall loss = 0.0403 and accuracy of 0.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"testing\")\n",
    "_,_,y_pred=run_model(sess,y_out,mean_loss,x_test,y_test,1,64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
